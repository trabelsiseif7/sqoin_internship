{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-31T19:48:25.663257Z","iopub.execute_input":"2023-07-31T19:48:25.664451Z","iopub.status.idle":"2023-07-31T19:48:55.895760Z","shell.execute_reply.started":"2023-07-31T19:48:25.664405Z","shell.execute_reply":"2023-07-31T19:48:55.893926Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.cuda.amp import custom_fwd, custom_bwd \n\nfrom bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:48:55.898181Z","iopub.execute_input":"2023-07-31T19:48:55.899020Z","iopub.status.idle":"2023-07-31T19:49:02.621923Z","shell.execute_reply.started":"2023-07-31T19:48:55.898980Z","shell.execute_reply":"2023-07-31T19:49:02.620744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n \n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output_cloned = torch.clone(output + self.adapter(input))\n            return output_cloned\n        else :\n            return output\n \n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n \n \nclass DequantizeAndLinear(torch.autograd.Function): \n    @staticmethod\n    @custom_fwd\n    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n \n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_quantized, absmax, code = ctx.saved_tensors\n        # grad_output: [*batch, out_features]\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        grad_input = grad_output @ weights_deq\n        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n        return grad_input, None, None, None, grad_bias\n \n \nclass FrozenBNBEmbedding(nn.Module):\n    def __init__(self, weight, absmax, code):\n        super().__init__()\n        self.num_embeddings, self.embedding_dim = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n \n    def forward(self, input, **kwargs):\n        with torch.no_grad():\n            # note: both quantuized weights and input indices are *not* differentiable\n            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n            output = F.embedding(input, weight_deq, **kwargs)\n        if self.adapter:\n            \n            output_cloned = torch.clone(output + self.adapter(input))\n            return output_cloned\n        else :\n            return output \n \n    @classmethod\n    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n        return cls(weights_int8, *state)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n \n \ndef quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n    assert chunk_size % 4096 == 0\n    code = None\n    chunks = []\n    absmaxes = []\n    flat_tensor = matrix.view(-1)\n    for i in range((matrix.numel() - 1) // chunk_size + 1):\n        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n        chunks.append(quantized_chunk)\n        absmaxes.append(absmax_chunk)\n \n    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n    absmax = torch.cat(absmaxes)\n    return matrix_i8, (absmax, code)\n \n \ndef convert_to_int8(model):\n    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n    for module in list(model.modules()):\n        for name, child in module.named_children():\n            if isinstance(child, nn.Linear):\n                print(name, child)\n                setattr( \n                    module,\n                    name,\n                    FrozenBNBLinear(\n                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                        bias=child.bias,\n                    ),\n                )\n            elif isinstance(child, nn.Embedding):\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBEmbedding(\n                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                    )\n                )\n     ","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:49:02.624072Z","iopub.execute_input":"2023-07-31T19:49:02.625164Z","iopub.status.idle":"2023-07-31T19:49:02.670411Z","shell.execute_reply.started":"2023-07-31T19:49:02.625124Z","shell.execute_reply":"2023-07-31T19:49:02.669274Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n    def __init__(self, config):\n        super().__init__(config)\n\n        convert_to_int8(self.attn)\n        convert_to_int8(self.mlp)\n\n\nclass GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n        \n\nclass GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\ntransformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:49:02.674627Z","iopub.execute_input":"2023-07-31T19:49:02.675499Z","iopub.status.idle":"2023-07-31T19:49:12.032972Z","shell.execute_reply.started":"2023-07-31T19:49:02.675375Z","shell.execute_reply":"2023-07-31T19:49:12.032032Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:49:12.034496Z","iopub.execute_input":"2023-07-31T19:49:12.034869Z","iopub.status.idle":"2023-07-31T19:49:13.865776Z","shell.execute_reply.started":"2023-07-31T19:49:12.034830Z","shell.execute_reply":"2023-07-31T19:49:13.864559Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c000cfb22464b7982b89b8d00b37363"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047128fbabb048c3a91c4e45df0b151c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc767bbb337e4f34a67c2786c637f86c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ee4285f724437680658cc7a39a109c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab78a0683cc46a29147f2e49454e454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b499dcc7d2415f83cbaefff24a1caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836569f9554f49c2a43b9835747bdf6c"}},"metadata":{}}]},{"cell_type":"code","source":"gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n\nif torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"  \ndevice = torch.device(dev)  \n\ngpt.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:49:13.869559Z","iopub.execute_input":"2023-07-31T19:49:13.869913Z","iopub.status.idle":"2023-07-31T19:51:04.208009Z","shell.execute_reply.started":"2023-07-31T19:49:13.869877Z","shell.execute_reply":"2023-07-31T19:51:04.205541Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f5070575c044d8a280f78f5d033163"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6facfae3986f48ad819ead2c20c9a0fd"}},"metadata":{}},{"name":"stdout","text":"k_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nk_proj Linear(in_features=4096, out_features=4096, bias=False)\nv_proj Linear(in_features=4096, out_features=4096, bias=False)\nq_proj Linear(in_features=4096, out_features=4096, bias=False)\nout_proj Linear(in_features=4096, out_features=4096, bias=False)\nfc_in Linear(in_features=4096, out_features=16384, bias=True)\nfc_out Linear(in_features=16384, out_features=4096, bias=True)\nlm_head Linear(in_features=4096, out_features=50400, bias=True)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"GPTJForCausalLM(\n  (transformer): GPTJModel(\n    (wte): FrozenBNBEmbedding(50400, 4096)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-27): 28 x GPTJBlock(\n        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTJAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): FrozenBNBLinear(4096, 4096)\n          (v_proj): FrozenBNBLinear(4096, 4096)\n          (q_proj): FrozenBNBLinear(4096, 4096)\n          (out_proj): FrozenBNBLinear(4096, 4096)\n        )\n        (mlp): GPTJMLP(\n          (fc_in): FrozenBNBLinear(4096, 16384)\n          (fc_out): FrozenBNBLinear(16384, 4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): FrozenBNBLinear(4096, 50400)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def add_adapters(model, adapter_dim=4, p = 0.1):\n    assert adapter_dim > 0\n\n    for name, module in model.named_modules():\n      if isinstance(module, FrozenBNBLinear):\n          if \"attn\" in name or \"mlp\" in name or \"head\" in name:\n              print(\"Adding adapter to\", name)\n              module.adapter = nn.Sequential(\n                nn.Linear(module.in_features, adapter_dim, bias=False),\n                nn.Dropout(p=p),\n                nn.Linear(adapter_dim, module.out_features, bias=False),\n            )\n              print(\"Initializing\", name)\n              nn.init.zeros_(module.adapter[2].weight)\n\n          else:\n              print(\"Not adding adapter to\", name)\n      elif isinstance(module, FrozenBNBEmbedding):\n          print(\"Adding adapter to\", name)\n          module.adapter = nn.Sequential(\n                nn.Embedding(module.num_embeddings, adapter_dim),\n                nn.Dropout(p=p),\n                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n            )\n          print(\"Initializing\", name)\n          nn.init.zeros_(module.adapter[2].weight)\n\nadd_adapters(gpt)\ngpt.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:04.214409Z","iopub.execute_input":"2023-07-31T19:51:04.214945Z","iopub.status.idle":"2023-07-31T19:51:04.380629Z","shell.execute_reply.started":"2023-07-31T19:51:04.214907Z","shell.execute_reply":"2023-07-31T19:51:04.379687Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Adding adapter to transformer.wte\nInitializing transformer.wte\nAdding adapter to transformer.h.0.attn.k_proj\nInitializing transformer.h.0.attn.k_proj\nAdding adapter to transformer.h.0.attn.v_proj\nInitializing transformer.h.0.attn.v_proj\nAdding adapter to transformer.h.0.attn.q_proj\nInitializing transformer.h.0.attn.q_proj\nAdding adapter to transformer.h.0.attn.out_proj\nInitializing transformer.h.0.attn.out_proj\nAdding adapter to transformer.h.0.mlp.fc_in\nInitializing transformer.h.0.mlp.fc_in\nAdding adapter to transformer.h.0.mlp.fc_out\nInitializing transformer.h.0.mlp.fc_out\nAdding adapter to transformer.h.1.attn.k_proj\nInitializing transformer.h.1.attn.k_proj\nAdding adapter to transformer.h.1.attn.v_proj\nInitializing transformer.h.1.attn.v_proj\nAdding adapter to transformer.h.1.attn.q_proj\nInitializing transformer.h.1.attn.q_proj\nAdding adapter to transformer.h.1.attn.out_proj\nInitializing transformer.h.1.attn.out_proj\nAdding adapter to transformer.h.1.mlp.fc_in\nInitializing transformer.h.1.mlp.fc_in\nAdding adapter to transformer.h.1.mlp.fc_out\nInitializing transformer.h.1.mlp.fc_out\nAdding adapter to transformer.h.2.attn.k_proj\nInitializing transformer.h.2.attn.k_proj\nAdding adapter to transformer.h.2.attn.v_proj\nInitializing transformer.h.2.attn.v_proj\nAdding adapter to transformer.h.2.attn.q_proj\nInitializing transformer.h.2.attn.q_proj\nAdding adapter to transformer.h.2.attn.out_proj\nInitializing transformer.h.2.attn.out_proj\nAdding adapter to transformer.h.2.mlp.fc_in\nInitializing transformer.h.2.mlp.fc_in\nAdding adapter to transformer.h.2.mlp.fc_out\nInitializing transformer.h.2.mlp.fc_out\nAdding adapter to transformer.h.3.attn.k_proj\nInitializing transformer.h.3.attn.k_proj\nAdding adapter to transformer.h.3.attn.v_proj\nInitializing transformer.h.3.attn.v_proj\nAdding adapter to transformer.h.3.attn.q_proj\nInitializing transformer.h.3.attn.q_proj\nAdding adapter to transformer.h.3.attn.out_proj\nInitializing transformer.h.3.attn.out_proj\nAdding adapter to transformer.h.3.mlp.fc_in\nInitializing transformer.h.3.mlp.fc_in\nAdding adapter to transformer.h.3.mlp.fc_out\nInitializing transformer.h.3.mlp.fc_out\nAdding adapter to transformer.h.4.attn.k_proj\nInitializing transformer.h.4.attn.k_proj\nAdding adapter to transformer.h.4.attn.v_proj\nInitializing transformer.h.4.attn.v_proj\nAdding adapter to transformer.h.4.attn.q_proj\nInitializing transformer.h.4.attn.q_proj\nAdding adapter to transformer.h.4.attn.out_proj\nInitializing transformer.h.4.attn.out_proj\nAdding adapter to transformer.h.4.mlp.fc_in\nInitializing transformer.h.4.mlp.fc_in\nAdding adapter to transformer.h.4.mlp.fc_out\nInitializing transformer.h.4.mlp.fc_out\nAdding adapter to transformer.h.5.attn.k_proj\nInitializing transformer.h.5.attn.k_proj\nAdding adapter to transformer.h.5.attn.v_proj\nInitializing transformer.h.5.attn.v_proj\nAdding adapter to transformer.h.5.attn.q_proj\nInitializing transformer.h.5.attn.q_proj\nAdding adapter to transformer.h.5.attn.out_proj\nInitializing transformer.h.5.attn.out_proj\nAdding adapter to transformer.h.5.mlp.fc_in\nInitializing transformer.h.5.mlp.fc_in\nAdding adapter to transformer.h.5.mlp.fc_out\nInitializing transformer.h.5.mlp.fc_out\nAdding adapter to transformer.h.6.attn.k_proj\nInitializing transformer.h.6.attn.k_proj\nAdding adapter to transformer.h.6.attn.v_proj\nInitializing transformer.h.6.attn.v_proj\nAdding adapter to transformer.h.6.attn.q_proj\nInitializing transformer.h.6.attn.q_proj\nAdding adapter to transformer.h.6.attn.out_proj\nInitializing transformer.h.6.attn.out_proj\nAdding adapter to transformer.h.6.mlp.fc_in\nInitializing transformer.h.6.mlp.fc_in\nAdding adapter to transformer.h.6.mlp.fc_out\nInitializing transformer.h.6.mlp.fc_out\nAdding adapter to transformer.h.7.attn.k_proj\nInitializing transformer.h.7.attn.k_proj\nAdding adapter to transformer.h.7.attn.v_proj\nInitializing transformer.h.7.attn.v_proj\nAdding adapter to transformer.h.7.attn.q_proj\nInitializing transformer.h.7.attn.q_proj\nAdding adapter to transformer.h.7.attn.out_proj\nInitializing transformer.h.7.attn.out_proj\nAdding adapter to transformer.h.7.mlp.fc_in\nInitializing transformer.h.7.mlp.fc_in\nAdding adapter to transformer.h.7.mlp.fc_out\nInitializing transformer.h.7.mlp.fc_out\nAdding adapter to transformer.h.8.attn.k_proj\nInitializing transformer.h.8.attn.k_proj\nAdding adapter to transformer.h.8.attn.v_proj\nInitializing transformer.h.8.attn.v_proj\nAdding adapter to transformer.h.8.attn.q_proj\nInitializing transformer.h.8.attn.q_proj\nAdding adapter to transformer.h.8.attn.out_proj\nInitializing transformer.h.8.attn.out_proj\nAdding adapter to transformer.h.8.mlp.fc_in\nInitializing transformer.h.8.mlp.fc_in\nAdding adapter to transformer.h.8.mlp.fc_out\nInitializing transformer.h.8.mlp.fc_out\nAdding adapter to transformer.h.9.attn.k_proj\nInitializing transformer.h.9.attn.k_proj\nAdding adapter to transformer.h.9.attn.v_proj\nInitializing transformer.h.9.attn.v_proj\nAdding adapter to transformer.h.9.attn.q_proj\nInitializing transformer.h.9.attn.q_proj\nAdding adapter to transformer.h.9.attn.out_proj\nInitializing transformer.h.9.attn.out_proj\nAdding adapter to transformer.h.9.mlp.fc_in\nInitializing transformer.h.9.mlp.fc_in\nAdding adapter to transformer.h.9.mlp.fc_out\nInitializing transformer.h.9.mlp.fc_out\nAdding adapter to transformer.h.10.attn.k_proj\nInitializing transformer.h.10.attn.k_proj\nAdding adapter to transformer.h.10.attn.v_proj\nInitializing transformer.h.10.attn.v_proj\nAdding adapter to transformer.h.10.attn.q_proj\nInitializing transformer.h.10.attn.q_proj\nAdding adapter to transformer.h.10.attn.out_proj\nInitializing transformer.h.10.attn.out_proj\nAdding adapter to transformer.h.10.mlp.fc_in\nInitializing transformer.h.10.mlp.fc_in\nAdding adapter to transformer.h.10.mlp.fc_out\nInitializing transformer.h.10.mlp.fc_out\nAdding adapter to transformer.h.11.attn.k_proj\nInitializing transformer.h.11.attn.k_proj\nAdding adapter to transformer.h.11.attn.v_proj\nInitializing transformer.h.11.attn.v_proj\nAdding adapter to transformer.h.11.attn.q_proj\nInitializing transformer.h.11.attn.q_proj\nAdding adapter to transformer.h.11.attn.out_proj\nInitializing transformer.h.11.attn.out_proj\nAdding adapter to transformer.h.11.mlp.fc_in\nInitializing transformer.h.11.mlp.fc_in\nAdding adapter to transformer.h.11.mlp.fc_out\nInitializing transformer.h.11.mlp.fc_out\nAdding adapter to transformer.h.12.attn.k_proj\nInitializing transformer.h.12.attn.k_proj\nAdding adapter to transformer.h.12.attn.v_proj\nInitializing transformer.h.12.attn.v_proj\nAdding adapter to transformer.h.12.attn.q_proj\nInitializing transformer.h.12.attn.q_proj\nAdding adapter to transformer.h.12.attn.out_proj\nInitializing transformer.h.12.attn.out_proj\nAdding adapter to transformer.h.12.mlp.fc_in\nInitializing transformer.h.12.mlp.fc_in\nAdding adapter to transformer.h.12.mlp.fc_out\nInitializing transformer.h.12.mlp.fc_out\nAdding adapter to transformer.h.13.attn.k_proj\nInitializing transformer.h.13.attn.k_proj\nAdding adapter to transformer.h.13.attn.v_proj\nInitializing transformer.h.13.attn.v_proj\nAdding adapter to transformer.h.13.attn.q_proj\nInitializing transformer.h.13.attn.q_proj\nAdding adapter to transformer.h.13.attn.out_proj\nInitializing transformer.h.13.attn.out_proj\nAdding adapter to transformer.h.13.mlp.fc_in\nInitializing transformer.h.13.mlp.fc_in\nAdding adapter to transformer.h.13.mlp.fc_out\nInitializing transformer.h.13.mlp.fc_out\nAdding adapter to transformer.h.14.attn.k_proj\nInitializing transformer.h.14.attn.k_proj\nAdding adapter to transformer.h.14.attn.v_proj\nInitializing transformer.h.14.attn.v_proj\nAdding adapter to transformer.h.14.attn.q_proj\nInitializing transformer.h.14.attn.q_proj\nAdding adapter to transformer.h.14.attn.out_proj\nInitializing transformer.h.14.attn.out_proj\nAdding adapter to transformer.h.14.mlp.fc_in\nInitializing transformer.h.14.mlp.fc_in\nAdding adapter to transformer.h.14.mlp.fc_out\nInitializing transformer.h.14.mlp.fc_out\nAdding adapter to transformer.h.15.attn.k_proj\nInitializing transformer.h.15.attn.k_proj\nAdding adapter to transformer.h.15.attn.v_proj\nInitializing transformer.h.15.attn.v_proj\nAdding adapter to transformer.h.15.attn.q_proj\nInitializing transformer.h.15.attn.q_proj\nAdding adapter to transformer.h.15.attn.out_proj\nInitializing transformer.h.15.attn.out_proj\nAdding adapter to transformer.h.15.mlp.fc_in\nInitializing transformer.h.15.mlp.fc_in\nAdding adapter to transformer.h.15.mlp.fc_out\nInitializing transformer.h.15.mlp.fc_out\nAdding adapter to transformer.h.16.attn.k_proj\nInitializing transformer.h.16.attn.k_proj\nAdding adapter to transformer.h.16.attn.v_proj\nInitializing transformer.h.16.attn.v_proj\nAdding adapter to transformer.h.16.attn.q_proj\nInitializing transformer.h.16.attn.q_proj\nAdding adapter to transformer.h.16.attn.out_proj\nInitializing transformer.h.16.attn.out_proj\nAdding adapter to transformer.h.16.mlp.fc_in\nInitializing transformer.h.16.mlp.fc_in\nAdding adapter to transformer.h.16.mlp.fc_out\nInitializing transformer.h.16.mlp.fc_out\nAdding adapter to transformer.h.17.attn.k_proj\nInitializing transformer.h.17.attn.k_proj\nAdding adapter to transformer.h.17.attn.v_proj\nInitializing transformer.h.17.attn.v_proj\nAdding adapter to transformer.h.17.attn.q_proj\nInitializing transformer.h.17.attn.q_proj\nAdding adapter to transformer.h.17.attn.out_proj\nInitializing transformer.h.17.attn.out_proj\nAdding adapter to transformer.h.17.mlp.fc_in\nInitializing transformer.h.17.mlp.fc_in\nAdding adapter to transformer.h.17.mlp.fc_out\nInitializing transformer.h.17.mlp.fc_out\nAdding adapter to transformer.h.18.attn.k_proj\nInitializing transformer.h.18.attn.k_proj\nAdding adapter to transformer.h.18.attn.v_proj\nInitializing transformer.h.18.attn.v_proj\nAdding adapter to transformer.h.18.attn.q_proj\nInitializing transformer.h.18.attn.q_proj\nAdding adapter to transformer.h.18.attn.out_proj\nInitializing transformer.h.18.attn.out_proj\nAdding adapter to transformer.h.18.mlp.fc_in\nInitializing transformer.h.18.mlp.fc_in\nAdding adapter to transformer.h.18.mlp.fc_out\nInitializing transformer.h.18.mlp.fc_out\nAdding adapter to transformer.h.19.attn.k_proj\nInitializing transformer.h.19.attn.k_proj\nAdding adapter to transformer.h.19.attn.v_proj\nInitializing transformer.h.19.attn.v_proj\nAdding adapter to transformer.h.19.attn.q_proj\nInitializing transformer.h.19.attn.q_proj\nAdding adapter to transformer.h.19.attn.out_proj\nInitializing transformer.h.19.attn.out_proj\nAdding adapter to transformer.h.19.mlp.fc_in\nInitializing transformer.h.19.mlp.fc_in\nAdding adapter to transformer.h.19.mlp.fc_out\nInitializing transformer.h.19.mlp.fc_out\nAdding adapter to transformer.h.20.attn.k_proj\nInitializing transformer.h.20.attn.k_proj\nAdding adapter to transformer.h.20.attn.v_proj\nInitializing transformer.h.20.attn.v_proj\nAdding adapter to transformer.h.20.attn.q_proj\nInitializing transformer.h.20.attn.q_proj\nAdding adapter to transformer.h.20.attn.out_proj\nInitializing transformer.h.20.attn.out_proj\nAdding adapter to transformer.h.20.mlp.fc_in\nInitializing transformer.h.20.mlp.fc_in\nAdding adapter to transformer.h.20.mlp.fc_out\nInitializing transformer.h.20.mlp.fc_out\nAdding adapter to transformer.h.21.attn.k_proj\nInitializing transformer.h.21.attn.k_proj\nAdding adapter to transformer.h.21.attn.v_proj\nInitializing transformer.h.21.attn.v_proj\nAdding adapter to transformer.h.21.attn.q_proj\nInitializing transformer.h.21.attn.q_proj\nAdding adapter to transformer.h.21.attn.out_proj\nInitializing transformer.h.21.attn.out_proj\nAdding adapter to transformer.h.21.mlp.fc_in\nInitializing transformer.h.21.mlp.fc_in\nAdding adapter to transformer.h.21.mlp.fc_out\nInitializing transformer.h.21.mlp.fc_out\nAdding adapter to transformer.h.22.attn.k_proj\nInitializing transformer.h.22.attn.k_proj\nAdding adapter to transformer.h.22.attn.v_proj\nInitializing transformer.h.22.attn.v_proj\nAdding adapter to transformer.h.22.attn.q_proj\nInitializing transformer.h.22.attn.q_proj\nAdding adapter to transformer.h.22.attn.out_proj\nInitializing transformer.h.22.attn.out_proj\nAdding adapter to transformer.h.22.mlp.fc_in\nInitializing transformer.h.22.mlp.fc_in\nAdding adapter to transformer.h.22.mlp.fc_out\nInitializing transformer.h.22.mlp.fc_out\nAdding adapter to transformer.h.23.attn.k_proj\nInitializing transformer.h.23.attn.k_proj\nAdding adapter to transformer.h.23.attn.v_proj\nInitializing transformer.h.23.attn.v_proj\nAdding adapter to transformer.h.23.attn.q_proj\nInitializing transformer.h.23.attn.q_proj\nAdding adapter to transformer.h.23.attn.out_proj\nInitializing transformer.h.23.attn.out_proj\nAdding adapter to transformer.h.23.mlp.fc_in\nInitializing transformer.h.23.mlp.fc_in\nAdding adapter to transformer.h.23.mlp.fc_out\nInitializing transformer.h.23.mlp.fc_out\nAdding adapter to transformer.h.24.attn.k_proj\nInitializing transformer.h.24.attn.k_proj\nAdding adapter to transformer.h.24.attn.v_proj\nInitializing transformer.h.24.attn.v_proj\nAdding adapter to transformer.h.24.attn.q_proj\nInitializing transformer.h.24.attn.q_proj\nAdding adapter to transformer.h.24.attn.out_proj\nInitializing transformer.h.24.attn.out_proj\nAdding adapter to transformer.h.24.mlp.fc_in\nInitializing transformer.h.24.mlp.fc_in\nAdding adapter to transformer.h.24.mlp.fc_out\nInitializing transformer.h.24.mlp.fc_out\nAdding adapter to transformer.h.25.attn.k_proj\nInitializing transformer.h.25.attn.k_proj\nAdding adapter to transformer.h.25.attn.v_proj\nInitializing transformer.h.25.attn.v_proj\nAdding adapter to transformer.h.25.attn.q_proj\nInitializing transformer.h.25.attn.q_proj\nAdding adapter to transformer.h.25.attn.out_proj\nInitializing transformer.h.25.attn.out_proj\nAdding adapter to transformer.h.25.mlp.fc_in\nInitializing transformer.h.25.mlp.fc_in\nAdding adapter to transformer.h.25.mlp.fc_out\nInitializing transformer.h.25.mlp.fc_out\nAdding adapter to transformer.h.26.attn.k_proj\nInitializing transformer.h.26.attn.k_proj\nAdding adapter to transformer.h.26.attn.v_proj\nInitializing transformer.h.26.attn.v_proj\nAdding adapter to transformer.h.26.attn.q_proj\nInitializing transformer.h.26.attn.q_proj\nAdding adapter to transformer.h.26.attn.out_proj\nInitializing transformer.h.26.attn.out_proj\nAdding adapter to transformer.h.26.mlp.fc_in\nInitializing transformer.h.26.mlp.fc_in\nAdding adapter to transformer.h.26.mlp.fc_out\nInitializing transformer.h.26.mlp.fc_out\nAdding adapter to transformer.h.27.attn.k_proj\nInitializing transformer.h.27.attn.k_proj\nAdding adapter to transformer.h.27.attn.v_proj\nInitializing transformer.h.27.attn.v_proj\nAdding adapter to transformer.h.27.attn.q_proj\nInitializing transformer.h.27.attn.q_proj\nAdding adapter to transformer.h.27.attn.out_proj\nInitializing transformer.h.27.attn.out_proj\nAdding adapter to transformer.h.27.mlp.fc_in\nInitializing transformer.h.27.mlp.fc_in\nAdding adapter to transformer.h.27.mlp.fc_out\nInitializing transformer.h.27.mlp.fc_out\nAdding adapter to lm_head\nInitializing lm_head\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"GPTJForCausalLM(\n  (transformer): GPTJModel(\n    (wte): FrozenBNBEmbedding(50400, 4096)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-27): 28 x GPTJBlock(\n        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n        (attn): GPTJAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): FrozenBNBLinear(4096, 4096)\n          (v_proj): FrozenBNBLinear(4096, 4096)\n          (q_proj): FrozenBNBLinear(4096, 4096)\n          (out_proj): FrozenBNBLinear(4096, 4096)\n        )\n        (mlp): GPTJMLP(\n          (fc_in): FrozenBNBLinear(4096, 16384)\n          (fc_out): FrozenBNBLinear(16384, 4096)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): FrozenBNBLinear(4096, 50400)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\n# Load the data\ndata = pd.read_csv('/kaggle/input/quora-data/data_topic_blockchain.csv')\ndata \n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:04.382124Z","iopub.execute_input":"2023-07-31T19:51:04.384124Z","iopub.status.idle":"2023-07-31T19:51:04.996918Z","shell.execute_reply.started":"2023-07-31T19:51:04.384087Z","shell.execute_reply":"2023-07-31T19:51:04.995799Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                               Question  \\\n0         Which branch of CS does Blockchain fall into?   \n1         Which branch of CS does Blockchain fall into?   \n2         Which branch of CS does Blockchain fall into?   \n3         Which branch of CS does Blockchain fall into?   \n4                                 What is a blockchain?   \n...                                                 ...   \n2854   What are some interesting research topics per...   \n2855        Why are companies interested in blockchain?   \n2856   What are the implications of a world where ev...   \n2857   Why are people only interested in bitcoins an...   \n2858  Would \"the blockchain\" and everything on it di...   \n\n                                                 Answer  \n0     Blockchain technology falls primarily within t...  \n1     Blockchain technology falls into the branch of...  \n2     It’s a whole new branch I can say…Many new thi...  \n3     Blockchain comprises of fundamentals of crypto...  \n4     I learned about blockchain in 2012, from the m...  \n...                                                 ...  \n2854  There are a lot of interesting topics to look ...  \n2855  There came many new technologies in the past t...  \n2856  A world where everything is connected on block...  \n2857  Bitcoin and blockchain are often used intercha...  \n2858  Nope. Once the internet went back up things wo...  \n\n[2859 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain technology falls primarily within t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain technology falls into the branch of...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>It’s a whole new branch I can say…Many new thi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain comprises of fundamentals of crypto...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is a blockchain?</td>\n      <td>I learned about blockchain in 2012, from the m...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2854</th>\n      <td>What are some interesting research topics per...</td>\n      <td>There are a lot of interesting topics to look ...</td>\n    </tr>\n    <tr>\n      <th>2855</th>\n      <td>Why are companies interested in blockchain?</td>\n      <td>There came many new technologies in the past t...</td>\n    </tr>\n    <tr>\n      <th>2856</th>\n      <td>What are the implications of a world where ev...</td>\n      <td>A world where everything is connected on block...</td>\n    </tr>\n    <tr>\n      <th>2857</th>\n      <td>Why are people only interested in bitcoins an...</td>\n      <td>Bitcoin and blockchain are often used intercha...</td>\n    </tr>\n    <tr>\n      <th>2858</th>\n      <td>Would \"the blockchain\" and everything on it di...</td>\n      <td>Nope. Once the internet went back up things wo...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2859 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#data.drop(\"Unnamed: 0\" , axis = 1 , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:04.998726Z","iopub.execute_input":"2023-07-31T19:51:04.999152Z","iopub.status.idle":"2023-07-31T19:51:05.003844Z","shell.execute_reply.started":"2023-07-31T19:51:04.999113Z","shell.execute_reply":"2023-07-31T19:51:05.002797Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:05.010294Z","iopub.execute_input":"2023-07-31T19:51:05.011436Z","iopub.status.idle":"2023-07-31T19:51:05.023125Z","shell.execute_reply.started":"2023-07-31T19:51:05.011398Z","shell.execute_reply":"2023-07-31T19:51:05.021810Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                        Question  \\\n0  Which branch of CS does Blockchain fall into?   \n1  Which branch of CS does Blockchain fall into?   \n2  Which branch of CS does Blockchain fall into?   \n3  Which branch of CS does Blockchain fall into?   \n4                          What is a blockchain?   \n\n                                              Answer  \n0  Blockchain technology falls primarily within t...  \n1  Blockchain technology falls into the branch of...  \n2  It’s a whole new branch I can say…Many new thi...  \n3  Blockchain comprises of fundamentals of crypto...  \n4  I learned about blockchain in 2012, from the m...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain technology falls primarily within t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain technology falls into the branch of...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>It’s a whole new branch I can say…Many new thi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Which branch of CS does Blockchain fall into?</td>\n      <td>Blockchain comprises of fundamentals of crypto...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is a blockchain?</td>\n      <td>I learned about blockchain in 2012, from the m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"prompt = []\nfor i in data.index:\n    # Update the value in the \"prompt\" column by concatenating strings\n    prompt.append(f\"\"\"[Question] : {data['Question'][i]} \\n[Response]:{data['Answer'][i]}\"\"\")\n\n# Access the updated value in the \"prompt\" column for a specific row\nprint(prompt[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:05.025077Z","iopub.execute_input":"2023-07-31T19:51:05.025727Z","iopub.status.idle":"2023-07-31T19:51:05.093799Z","shell.execute_reply.started":"2023-07-31T19:51:05.025690Z","shell.execute_reply":"2023-07-31T19:51:05.092772Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[Question] : Which branch of CS does Blockchain fall into? \n[Response]:Blockchain technology falls primarily within the domain of Computer Science and its various subfields. Specifically, blockchain technology encompasses concepts and techniques related to distributed systems, cryptography, data structures, networking, and consensus algorithms. Here are a few specific branches of Computer Science that are relevant to blockchain: 1. Distributed Systems: Blockchain is fundamentally a decentralized distributed system. Research and concepts in distributed systems, including peer-to-peer networks, fault tolerance, consensus protocols, and data replication, are crucial Continue ReadingBlockchain technology falls primarily within the domain of Computer Science and its various subfields. Specifically, blockchain technology encompasses concepts and techniques related to distributed systems, cryptography, data structures, networking, and consensus algorithms. Here are a few specific branches of Computer Science that are relevant to blockchain: 1. Distributed Systems: Blockchain is fundamentally a decentralized distributed system. Research and concepts in distributed systems, including peer-to-peer networks, fault tolerance, consensus protocols, and data replication, are crucial for understanding and designing blockchain systems. 2. Cryptography: Cryptography plays a vital role in securing blockchain networks. Knowledge of cryptographic algorithms, cryptographic primitives (such as hash functions and digital signatures), and cryptographic protocols is essential to understand the security mechanisms employed within blockchains. 3. Data Structures: Blockchains rely on specific data structures to store and organize data. Concepts from data structures, such as linked lists, hash tables, Merkle trees, and various types of ledgers (e.g., UTXO or account-based), are relevant to blockchain development and optimization. 4. Networking and Protocol Design: Blockchain networks involve communication and consensus among distributed nodes. Understanding networking protocols, network architecture, peer-to-peer networking, and network security is important for designing efficient and secure blockchain protocols. 5. Algorithms and Data Analytics: Blockchain systems involve various algorithms and techniques for transaction validation, consensus mechanisms (e.g., Proof-of-Work, Proof-of-Stake), mining, and data analysis. Knowledge of algorithm design and analysis, computational complexity, and data analytics can provide insights into the performance and optimization of blockchain systems. It's worth noting that blockchain is an interdisciplinary field that combines elements of computer science, economics, cryptography, game theory, and more. As such, professionals from various disciplines collaborate to advance the understanding and application of blockchain technology.\n","output_type":"stream"}]},{"cell_type":"code","source":"data[\"prompt\"] = prompt\ndata = data[\"prompt\"]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:05.095285Z","iopub.execute_input":"2023-07-31T19:51:05.095601Z","iopub.status.idle":"2023-07-31T19:51:05.102295Z","shell.execute_reply.started":"2023-07-31T19:51:05.095570Z","shell.execute_reply":"2023-07-31T19:51:05.100171Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.1) \ntrain.to_csv('/train.csv', index=False)\ntest.to_csv('/test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:05.103812Z","iopub.execute_input":"2023-07-31T19:51:05.104617Z","iopub.status.idle":"2023-07-31T19:51:05.407414Z","shell.execute_reply.started":"2023-07-31T19:51:05.104563Z","shell.execute_reply":"2023-07-31T19:51:05.406382Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('csv', data_files={'train': '/train.csv',\n                                              'test': '/test.csv'})","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:05.408758Z","iopub.execute_input":"2023-07-31T19:51:05.409132Z","iopub.status.idle":"2023-07-31T19:51:06.417154Z","shell.execute_reply.started":"2023-07-31T19:51:05.409098Z","shell.execute_reply":"2023-07-31T19:51:06.416143Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1171c22cfca1d619/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b26cb95de784e8e8bbdbd9ff1de7b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4fb423ddcd41cb916897218bffd6c0"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1171c22cfca1d619/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd60dc9f478481bac5190401adc4b24"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ndef tokenize_function(examples):\n    return tokenizer(examples[\"prompt\"], padding=True, truncation=True, max_length= 512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns([\"prompt\"])\ntokenized_datasets.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:06.418840Z","iopub.execute_input":"2023-07-31T19:51:06.419574Z","iopub.status.idle":"2023-07-31T19:51:09.862314Z","shell.execute_reply.started":"2023-07-31T19:51:06.419536Z","shell.execute_reply":"2023-07-31T19:51:09.861260Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6697a9f4a2b4e61ba42a9f6199173b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77641187105d4a3b947ac0d90d419db2"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nfull_train_dataset = tokenized_datasets[\"train\"]\ntrain_dataloader = DataLoader(full_train_dataset, shuffle=True, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:09.863793Z","iopub.execute_input":"2023-07-31T19:51:09.864813Z","iopub.status.idle":"2023-07-31T19:51:09.871381Z","shell.execute_reply.started":"2023-07-31T19:51:09.864773Z","shell.execute_reply":"2023-07-31T19:51:09.870112Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from bitsandbytes.optim import Adam8bit\n\ngpt.gradient_checkpointing_enable()\noptimizer = Adam8bit(gpt.parameters(), lr=1e-5, weight_decay=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:09.873632Z","iopub.execute_input":"2023-07-31T19:51:09.874091Z","iopub.status.idle":"2023-07-31T19:51:09.934440Z","shell.execute_reply.started":"2023-07-31T19:51:09.874053Z","shell.execute_reply":"2023-07-31T19:51:09.933423Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2\nnum_training_steps = num_epochs * len(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:09.936945Z","iopub.execute_input":"2023-07-31T19:51:09.937311Z","iopub.status.idle":"2023-07-31T19:51:09.945094Z","shell.execute_reply.started":"2023-07-31T19:51:09.937280Z","shell.execute_reply":"2023-07-31T19:51:09.943972Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"lr_scheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer, int(num_training_steps*0.1), num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:09.948702Z","iopub.execute_input":"2023-07-31T19:51:09.949019Z","iopub.status.idle":"2023-07-31T19:51:09.962619Z","shell.execute_reply.started":"2023-07-31T19:51:09.948993Z","shell.execute_reply":"2023-07-31T19:51:09.961672Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nscaler = torch.cuda.amp.GradScaler()\nprogress_bar = tqdm(range(num_training_steps))\ngpt.train()\ngpt.gradient_checkpointing_enable()\nk = 0\n\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        k = k + 1\n        if k % 500 == 0:\n          \n          #print(k)\n          state = {'k' : k, 'epoch': num_epochs, 'lr_scheduler': lr_scheduler.state_dict(), 'state_dict': gpt.state_dict(), 'optimizer': optimizer.state_dict()}\n          #torch.save(state, filepath)\n\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        optimizer.zero_grad()\n        \n\n        with torch.autograd.profiler.record_function(\"model_inference\"):\n            with torch.cuda.amp.autocast():\n                \n                out = gpt.forward(**batch,)\n                \n                loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n                                  reduction='mean', label_smoothing=0.1)\n\n        #print(loss)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        lr_scheduler.step()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T19:51:09.964154Z","iopub.execute_input":"2023-07-31T19:51:09.964692Z","iopub.status.idle":"2023-07-31T22:10:46.205560Z","shell.execute_reply.started":"2023-07-31T19:51:09.964659Z","shell.execute_reply":"2023-07-31T22:10:46.204426Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/644 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4ca72f6aab42ba89d19344d43871bf"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.save_pretrained('FineTune_gptj')\ntokenizer.save_pretrained('FineTune_gptj')\nimport shutil\nshutil.make_archive(\"FineTune_gptj\", 'zip', \"/kaggle/working/FineTune_gptj\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'FineTune_gptj.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation**","metadata":{}},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]:Are blockchain transactions slow? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:10:46.206999Z","iopub.execute_input":"2023-07-31T22:10:46.208003Z","iopub.status.idle":"2023-07-31T22:11:14.668151Z","shell.execute_reply.started":"2023-07-31T22:10:46.207962Z","shell.execute_reply":"2023-07-31T22:11:14.667043Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]:Are blockchain transactions slow? \n [Response] : Blockchain technology is based on a distributed ledger. The network of computers participating in the decentralized system makes it more secure and efficient than traditional systems. However, this also means that each transaction can take some time to process. It's possible for a large number of people or companies to use an exchange at once, making transactions faster than they are with other types of exchanges (including online banks). In addition, Blockchain transactions usually take longer than normal bank transfers because of security regulations.<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]: What’s the advantage of building games on a blockchain platform? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:11:14.669779Z","iopub.execute_input":"2023-07-31T22:11:14.670168Z","iopub.status.idle":"2023-07-31T22:12:00.364012Z","shell.execute_reply.started":"2023-07-31T22:11:14.670131Z","shell.execute_reply":"2023-07-31T22:12:00.362928Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]: What’s the advantage of building games on a blockchain platform? \n [Response] :  I believe that Blockchain can help game developers to do something we have not been able to before. And, in general, blockchain has the potential to improve everything: business, data storage and so forth. If you want to know more about how it could be used by gaming studios, my colleague at B2M will explain all in one day at Games Summit London. But for now, let me tell you about what blockchains are: they are distributed databases, but with three major features: decentralization, security and transparency. Decentralization means that nobody controls the entire database; only nodes ( computers ) can access and store information in it. Security is because blockchain is a digital ledger, which makes it tamper-resistant. Transparency ensures everyone sees and understands everything going into and out of an accounts<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]: Why is blockchain required for NFT? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:12:00.365499Z","iopub.execute_input":"2023-07-31T22:12:00.367139Z","iopub.status.idle":"2023-07-31T22:13:38.036789Z","shell.execute_reply.started":"2023-07-31T22:12:00.367096Z","shell.execute_reply":"2023-07-31T22:13:38.035773Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]: Why is blockchain required for NFT? \n [Response] :  There are a few reasons why Blockchain has been touted as the “killer” technology of 2019. It's not just that it can help secure cryptocurrencies, or that it's more efficient and secure than other forms of distributed ledger technology. In fact, many aspects of blockchain and Cryptocurrency seem like they would be enough to satisfy the demand that is in front us; but there's one key benefit from using Blockchain as a platform for non-fungible tokens (NFT) and the reasons behind the importance of this particular feature. If you have an asset in your possession and its value diminishes over time, then this is when NFTs could really come in handy. The value associated with digital collectibles often changes over their lifetimes, depending on who's holding the data in different places, how it's stored, and other factors. But if you're having problems tracking what happens to your collection of digital goods, the only way forward may be through blockchain. When dealing with large collections of items (like game codes, concert tickets, paintings, rare records), NFTs allow for each individual item to be recorded in one place, so if you want to find out more about an item or learn where it was sold, this information isn't scattered around various databases. Using a centralized database helps ensure consistency, transparency, security, and much more. If a record about your unique virtual property had been made public, any third party could potentially access or manipulate it if it wasn't adequately protected. This could lead to unintended consequences for your valuable collection. By recording it using blockchain, you're protecting your rights and interests and maintaining ownership even beyond your lifetime. That's an important aspect of NFT.<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]: What is a blockchain? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:13:38.038321Z","iopub.execute_input":"2023-07-31T22:13:38.040129Z","iopub.status.idle":"2023-07-31T22:14:47.933017Z","shell.execute_reply.started":"2023-07-31T22:13:38.040088Z","shell.execute_reply":"2023-07-31T22:14:47.931927Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]: What is a blockchain? \n [Response] : A Blockchain is a type of digital ledger system, that has no middlemen or centralized institutions involved. Here we will explain more about Blockchain in the easiest way possible: A Blockchain is a decentralized system where everyone has equal opportunity to join the network and create data/records, as well as to participate in transaction, mining, voting etc. Block chain consistsof blocks that store the information for every single record, transaction etc on the network. Each block contains a timestamp to verify its integrity, a setContinue Reading A Blockchain is a type ofdigital ledger system, that has no middlemen orcentralized institutions involved. Here we will explain moreabout blockchain in the easiest way possible: ABlockchain is a decentralized systemwhere everything is stored in its entirety across the network without anymiddlemen, such as banks or central servers, that may control the information. Therefore, there are no intermediaries between users and the records are immutable, which means they cannot be altered once the transaction has occurred. This helps to make sure that all transactions are recorded accurately without any possibility of fraud or manipulation. The whole process is transparently recorded on the Blockchain’s system which enables anyone on the network to validate it without needing permission from another person.<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]: Do pending blockchain transactions expire? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:14:47.934392Z","iopub.execute_input":"2023-07-31T22:14:47.934795Z","iopub.status.idle":"2023-07-31T22:15:02.152184Z","shell.execute_reply.started":"2023-07-31T22:14:47.934756Z","shell.execute_reply":"2023-07-31T22:15:02.151030Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]: Do pending blockchain transactions expire? \n [Response] :  Blockchain is a public ledger that records the state of the Bitcoin network. There are no expiration dates on transactions and the time required to complete the transaction can vary depending upon the network condition, but most blocks take approximately 10 minutes to process. \n\n<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"gpt.eval()\nwith torch.no_grad():\n  prompt = tokenizer(\"[Question]: What applications and uses will bring blockchain mainstream? \\n [Response] : \", truncation=True, padding=True, max_length=256, return_tensors='pt')\n  prompt = {key: value.to(device) for key, value in prompt.items()}\n  out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n  print(tokenizer.decode(out[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T22:15:02.153537Z","iopub.execute_input":"2023-07-31T22:15:02.153934Z","iopub.status.idle":"2023-07-31T22:15:43.875185Z","shell.execute_reply.started":"2023-07-31T22:15:02.153894Z","shell.execute_reply":"2023-07-31T22:15:43.874172Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[Question]: What applications and uses will bring blockchain mainstream? \n [Response] :  Blockchain, as it’s now used today in the crypto space (Bitcoin/Ethereum), is an inefficient way of storing data. Its underlying premise that a large number of people are participating on an open distributed ledger, without trust between them, means that there must be some central authority who can verify your identity when making purchases, etc. Blockchain technology provides many other benefits beyond the scope this question is discussing. For a more detailed discussion of these technologies, you should check out the following resources:  • Bitcoin whitepaper—The initial blockchain paper by Satoshi Nakamoto • The Hyperledger project — A set of block-building protocols for secure networks• Ethereum white papers—A nonfiction summary of the basic science behind cryptocurrency<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"while True : \n    responses = []\n    pred = []\n    gpt.eval()\n    for sentence in test.values:\n        #print(\"**************************************************************\")\n        st = sentence.split('[Response]:')[0].strip()\n        responses.append('[Response]:'+sentence.split('[Response]:')[1].strip())\n        #print(st)\n        with torch.no_grad():\n            prompt = tokenizer(st, truncation=True, padding=True, max_length=256, return_tensors='pt')\n            prompt = {key: value.to(device) for key, value in prompt.items()}\n            out = gpt.generate(**prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n            #print('\\n')\n            #print(\"GPT-J :\" , tokenizer.decode(out[0]))\n            pred.append(tokenizer.decode(out[0],skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-07-17T17:21:35.845099Z","iopub.execute_input":"2023-07-17T17:21:35.845447Z","iopub.status.idle":"2023-07-17T22:03:14.799983Z","shell.execute_reply.started":"2023-07-17T17:21:35.845405Z","shell.execute_reply":"2023-07-17T22:03:14.798573Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m13\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompt = tokenizer(st, truncation=\u001b[94mTrue\u001b[0m, padding=\u001b[94mTrue\u001b[0m, max_length=\u001b[94m256\u001b[0m, return    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompt = {key: value.to(device) \u001b[94mfor\u001b[0m key, value \u001b[95min\u001b[0m prompt.items()}               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m13 \u001b[2m│   │   │   \u001b[0mout = gpt.generate(**prompt, max_length=\u001b[94m512\u001b[0m, top_k=\u001b[94m50\u001b[0m, top_p=\u001b[94m0.9\u001b[0m, temperatur    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m#print('\\n')\u001b[0m                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m#print(\"GPT-J :\" , tokenizer.decode(out[0]))\u001b[0m                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   │   \u001b[0mpred.append(tokenizer.decode(out[\u001b[94m0\u001b[0m],skip_special_tokens=\u001b[94mTrue\u001b[0m))                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1572\u001b[0m in \u001b[92mgenerate\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1569 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1570 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1571 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 13. run sample\u001b[0m                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1572 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.sample(                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1573 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1574 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1575 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_warper=logits_warper,                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2619\u001b[0m in \u001b[92msample\u001b[0m          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2616 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2617 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2618 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2619 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2620 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2621 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2622 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_attentions=output_attentions,                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m854\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 852 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 854 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 855 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 856 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 857 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m689\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhead_mask[i],                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 687 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 688 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 689 \u001b[2m│   │   │   │   \u001b[0moutputs = block(                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 690 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states=hidden_states,                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 691 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_past=layer_past,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 692 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m309\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 306 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 307 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 308 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 309 \u001b[2m│   │   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states=hidden_states,                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 311 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past=layer_past,                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 312 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m257\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 254 \u001b[0m\u001b[2m│   │   │   \u001b[0mpresent = \u001b[94mNone\u001b[0m                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 255 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 256 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# compute self-attention: V x Softmax(QK^T)\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 257 \u001b[2m│   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._attn(query, key, value, attention_mask, head_m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 258 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 259 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m._merge_heads(attn_output, \u001b[96mself\u001b[0m.num_attention_heads, \u001b[96mself\u001b[0m.head  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 260 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m.out_proj(attn_output)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m161\u001b[0m in \u001b[92m_attn\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 158 \u001b[0m\u001b[2m│   │   \u001b[0mquery = query.to(torch.float32)                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 159 \u001b[0m\u001b[2m│   │   \u001b[0mkey = key.to(torch.float32)                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 160 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 161 \u001b[2m│   │   \u001b[0mattn_weights = torch.matmul(query, key.transpose(-\u001b[94m1\u001b[0m, -\u001b[94m2\u001b[0m))                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 162 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 163 \u001b[0m\u001b[2m│   │   \u001b[0mmask_value = torch.finfo(attn_weights.dtype).min                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar ty\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>prompt = tokenizer(st, truncation=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, max_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span>, return    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>prompt = {key: value.to(device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompt.items()}               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>13 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>out = gpt.generate(**prompt, max_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">512</span>, top_k=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>, top_p=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.9</span>, temperatur    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print('\\n')</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(\"GPT-J :\" , tokenizer.decode(out[0]))</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>pred.append(tokenizer.decode(out[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],skip_special_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>))                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1572</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1569 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 13. run sample</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1572 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sample(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1573 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1574 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>logits_warper=logits_warper,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2619</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">sample</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2617 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2618 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2619 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2620 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2621 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2622 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">854</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 851 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 852 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 854 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 855 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 856 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>past_key_values=past_key_values,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 857 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">689</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>head_mask[i],                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 687 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 688 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 689 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 690 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hidden_states=hidden_states,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 691 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 692 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">309</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 306 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 307 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 308 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 309 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 310 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_states=hidden_states,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 311 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>layer_past=layer_past,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">257</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>present = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># compute self-attention: V x Softmax(QK^T)</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 257 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._attn(query, key, value, attention_mask, head_m  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 259 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._merge_heads(attn_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_attention_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 260 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_proj(attn_output)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">161</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 158 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>query = query.to(torch.float32)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 159 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>key = key.to(torch.float32)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 161 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_weights = torch.matmul(query, key.transpose(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>))                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>mask_value = torch.finfo(attn_weights.dtype).min                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar ty</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\neval_dataset_gptj = pd.DataFrame({'paraphrase':paraphrases , 'predicted':pred})\neval_dataset_gptj.to_excel('/kaggle/working/eval_dataset_gptj.xlsx',index=False)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\n#torch.save(gpt.state_dict(), '/kaggle/working/gpt-j-6B.pt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"print(eval_dataset_gptj['paraphrase'][0])\nprint(eval_dataset_gptj['predicted'][0])\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"predicted = []\nfor p in eval_dataset_gptj.predicted.values :\n    #print(p)\n    st = p.split('[Positive]:')[1].strip()\n    predicted.append('[Positive]:'+st)\n    \"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicted[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#paraphrases[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"!pip install rouge\n!pip install evaluate\n!pip install rouge_score\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import nltk\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate import bleu_score\nfrom rouge import Rouge\nimport math\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Calculate the BLEU score\nreferences = [[nltk.word_tokenize(original)] for original in paraphrases]\nhypotheses = [nltk.word_tokenize(predicted) for predicted in predicted]\nbleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0.0, 0.0, 0.0), smoothing_function=SmoothingFunction().method1)\nbleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=SmoothingFunction().method1)\nbleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.0), smoothing_function=SmoothingFunction().method1)\nbleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1)\n\nprint(\"BLEU-1 score:\", bleu1)\nprint(\"BLEU-2 score:\", bleu2)\nprint(\"BLEU-3 score:\", bleu3)\nprint(\"BLEU-4 score:\", bleu4)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Calculate the Rouge score\nrouge = Rouge()\nscores = rouge.get_scores(predicted,paraphrases, avg=True)\nrouge_l = scores['rouge-l']\nprint(\"Rouge-L score:\", rouge_l)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import evaluate\nrouge = evaluate.load('rouge')\n\nresults = rouge.compute(predictions=predicted,references=paraphrases)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"results\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"bleu = evaluate.load(\"bleu\")\nresults = bleu.compute(predictions=predicted,references=paraphrases)\nprint(results)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[[\"text\",\"target\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}